---
title: "人工智能作业4"
date: 2019-12-20T20:56:03+08:00
draft: false
categories: ["人工智能"]
tags: ["AI", "EM算法", "图片分类"]
author: "Tifinity"
---

# 人工智能第四次作业 17343105 田皓

## EM算法实验内容

### 算法原理

EM **Expectation-maximization algorithm **期望最大化算法

a.记录了每次选择的是A还是B

A硬币出现正面的概率为 $P(A) = \frac{24}{24+6}= 0.80$

B硬币出现正面的概率为 $P(B) = \frac{9}{9+11}= 0.45$

b.没有记录每次选择的是A还是B

假设每次选择的硬币是X（X={A或B}），我们需要根据结果求PA，PB，就要知道X，而要用最大似然来估计X又需要知道PAPB，这是个死循环。

这个时候就需要使用EM算法，基本思想是：先初始化PA和PB，然后用PA和PB估计X，有了X再计算PA和PB，不断重复直到收敛。

### 算法步骤

1. 计算期望（E），利用对隐变量（即硬币是A还是B）的现有估计值，计算AB硬币的正反面次数；
2. 最大化（M），计算新的AB硬币正面概率，与上一次迭代结果比较，若收敛则结束，否则再次执行1。

### 算法结果

简单地取多组初值进行实验，结果有些许差别。阈值主要影响迭代次数，这个例子比较简单，一般十次迭代就能得到结果。详见data.txt。

   

## 图片分类任务

从[官网](http://www.cs.toronto.edu/~kriz/cifar.html)下载cifar-10数据集。

![image-20191205184617188](https://github.com/Tifinity/MyImage/raw/master/AI-Learning/hw4/image-20191205184629303.png)

### 作业要求:

- 明确图片分类任务的基本流程

  将数据集分为训练集和测试集，输入训练集训练模型，用验证集调整参数，估计模型准确率，输入测试集，得到结果，与标签对比得到准确率。

- 清楚数据集的训练集验证集和测试集的划分和用途,简单介绍自己的用
  法

  举个例子，我们是老师，程序是学生，那么：

  训练集是课本，学生根据课本里的内容来掌握知识；

  验证集是作业，通过作业我们可以了解学生对知识的掌握情况并调整教学方法；

  测试集是试卷，通过考试检验学生到底掌握的如何。

  验证集不是必须的，划分比例一般为6：2：2，三者本质上无区别，是为了更好的调整模型而存在的，比如防止过拟合。

- 编写 K 近邻算法,SVM 以及简单的两层神经网络分类器各一(多分类分
  类器)进行图片分类任务

  见下。

- 根据得到的分类结果(精度)说明和比较各个算法的优越性和局限性,
  懂得它们之间的差异

  见下。

- 编写实验报告

  正是本文档。

### KNN （K最近邻算法）

knn是机器学习中最简单的方法之一，对于一个样本，取特征空间中的k个与其最相似的样本，其中数量最多的类别就是该样本的类别。knn不需要估计参数，实际上也不需要训练，测试集的每一张图片都在训练集中找最相似的k个来预测即可。

算法步骤：

1. 对测试集每一个样本，计算其与训练集所有元素的“距离”；
2. 循环k次，找出前k个距离最小的样本；
3. 在这k个样本中找出最多的类别，作为预测类别；
4. 计算准确率。

算法优点：

1. 简单
2. 适合多分类问题

算法缺点：

1. 计算量大，时间长
2. 结果更多依据训练样本的比例，而不是数量

结果：

![image-20191204190855286](https://github.com/Tifinity/MyImage/raw/master/AI-Learning/hw4/image-20191204190855286.png)

### SVM （支持向量机）

SVM主要用于分类问题中，将每一个样本看作一个点在n维空间中(n是特征数，在这里就是图像的大小，即32\*32*3)，目的是找到分隔两个类的“最好”的超平面，“最好”指与其最近的点的距离最大。

在样本空间中，超平面可通过如下形式来描述：
$$
w^Tx + b
$$
w为法向量，决定平面的方向，b是偏移表示平面离原点的距离，任意点到超平面$(w，b)$的距离为：
$$
r = \frac{|w^Tx + b|}{\left \| w \right \|}
$$
现在开始寻找最大间隔，首先确定分类器为$w^Tx + b > 1$则$y = 1$，$w^Tx + b < -1$则$y = -1$

SVM多分类

SVM算法最初是为二值分类问题设计的，当处理多类问题时，就需要构造合适的多类分类器。目前，构造SVM多类分类器的方法主要有两类：

- 一类是直接法，直接在目标函数上进行修改，将多个分类面的参数求解合并到一个最优化问题中，通过求解该最优化问题“一次性”实现多类分类。这种方法看似简单，但其计算复杂度比较高，实现起来比较困难，只适合用于小型问题中；

- 另一类是间接法，主要是通过组合多个二分类器来实现多分类器的构造，常见的方法有one-against-one和one-against-all两种。

在这里我使用间接法，间接法又有：

- one-versus-rest：训练时依次把某个类别的样本归为一类,其他剩余的样本归为另一类，k个类别k个分类器。分类时将未知样本分类为具有最大分类函数值的那类。

- one-versus-one：其做法是在任意两类样本之间设计一个SVM，因此k个类别的样本就需要设计k(k-1)/2个SVM。当对一个未知样本进行分类时，最后得票最多的类别即为该未知样本的类别。



调用sklearn包的SVM函数实现

简单使用不同的参数实现，得到以下结果：

从上到下分别是：线性，rbf（gamma=0.1），rbf（gamma=100），poly（degree=3）

![image-20191205191035911](https://github.com/Tifinity/MyImage/raw/master/AI-Learning/hw4/image-20191205191035911.png)

### 简单神经网络

使用tensorflow2.0在docker中运行完成本次任务。

按照官网教程使用CNN，网络结构如下：

![image-20191204203020314](https://github.com/Tifinity/MyImage/raw/master/AI-Learning/hw4/image-20191204203020314.png)

从上到下依次为卷积层，池化层，卷积层，池化层，卷积层，压缩层，全链接层，全链接层。

图像中0-255的像素值先转换为0-1的浮点数，然后输入第一个卷积层，输入shape为高32像素，宽32像素，三通道，卷积核形状为3*3，32个。

Conv2D的每一次卷积变换如下：

1 1 1 0

0 1 1 1	=>  4 3

0 0 1 1           2 4

0 0 1 1

卷积核为：

1 0 1

0 1 0

1 0 1

MaxPooling2D最大池化层的每一次变换如下:

2*2取最大值

4  3  =>  4

2  4   

图像的宽和高逐步变小，最后通过Flatten将4\*4\*64的三维张量转化为1024的向量，再使用两个全连接层Dense构建一个输入层为1024，隐藏层为64，输出层为10的普通神经网络，最后输出的长度为10的向量代表cifar-10的10个类别，向量中第k个值代表输入图片分类为k的概率。

结果：

![image-20191204213642669](https://github.com/Tifinity/MyImage/raw/master/AI-Learning/hw4/image-20191204213642669.png)

准确率0.6696，比knn和svm都高出很多。